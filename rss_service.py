"""
üì∞ RSS Service - –ø–∞—Ä—Å–∏–Ω–≥ —Ç—Ä–µ–Ω–¥—ñ–≤ –∑ –¥–∏–∑–∞–π–Ω–µ—Ä—Å—å–∫–∏—Ö —Å–∞–π—Ç—ñ–≤
–î–∂–µ—Ä–µ–ª–∞ –¥–ª—è —ñ–¥–µ–π –∫–æ–Ω—Ç–µ–Ω—Ç—É –ø—Ä–æ –ø–ª–∏—Ç–∫—É, —Ä–µ–º–æ–Ω—Ç, –¥–∏–∑–∞–π–Ω –≤–∞–Ω–Ω–∏—Ö –∫—ñ–º–Ω–∞—Ç.
"""
import feedparser
from typing import List, Dict, Optional, Any
from datetime import datetime, timedelta
import re
import os
import json

# RSS-–ª–µ–Ω—Ç–∏ –∑ –¥–∏–∑–∞–π–Ω—É —Ç–∞ —Ä–µ–º–æ–Ω—Ç—É
RSS_FEEDS = {
    'dezeen': {
        'url': 'https://www.dezeen.com/interiors/feed/',
        'name': 'Dezeen Interiors',
        'category': 'design',
        'language': 'en'
    },
    'archdaily': {
        'url': 'https://www.archdaily.com/feed',
        'name': 'ArchDaily',
        'category': 'architecture',
        'language': 'en'
    },
    'houzz': {
        'url': 'https://www.houzz.com/rss/stories',
        'name': 'Houzz Stories',
        'category': 'home_design',
        'language': 'en'
    },
    'dwell': {
        'url': 'https://www.dwell.com/feed',
        'name': 'Dwell',
        'category': 'modern_design',
        'language': 'en'
    },
    'designboom': {
        'url': 'https://www.designboom.com/feed/',
        'name': 'Designboom',
        'category': 'design',
        'language': 'en'
    },
    'schoener_wohnen': {
        'url': 'https://www.schoener-wohnen.de/rss/news.xml',
        'name': 'Sch√∂ner Wohnen',
        'category': 'home_design',
        'language': 'de'
    }
}


def _normalize_feeds(parsed: Any) -> Dict[str, Dict]:
    """Normalize feeds input into the internal dict format."""
    # –§–æ—Ä–º–∞—Ç 1: {"key": {"url": "...", "name": "...", ...}, ...}
    if isinstance(parsed, dict):
        feeds: Dict[str, Dict] = {}
        for key, value in parsed.items():
            if isinstance(value, str):
                url = value.strip()
                if not url:
                    continue
                feeds[str(key)] = {
                    'url': url,
                    'name': str(key),
                    'category': 'custom',
                    'language': 'en'
                }
            elif isinstance(value, dict) and value.get('url'):
                url = str(value.get('url') or '').strip()
                if not url:
                    continue
                feeds[str(key)] = {
                    'url': url,
                    'name': value.get('name', str(key)),
                    'category': value.get('category', 'custom'),
                    'language': value.get('language', 'en')
                }
        return feeds

    # –§–æ—Ä–º–∞—Ç 2: ["https://example.com/feed.xml", ...]
    if isinstance(parsed, list):
        feeds = {}
        i = 0
        for raw_url in parsed:
            if not isinstance(raw_url, str):
                continue
            url = raw_url.strip()
            if not url:
                continue
            i += 1
            feeds[f'feed_{i}'] = {
                'url': url,
                'name': f'Feed {i}',
                'category': 'custom',
                'language': 'en'
            }
        return feeds

    return {}


def get_rss_feeds_config(user_id: Optional[str] = None) -> Dict[str, Dict]:
    """–û—Ç—Ä–∏–º–∞—Ç–∏ –∫–æ–Ω—Ñ—ñ–≥ RSS-–ª–µ–Ω—Ç: per-user (DB) ‚Üí env (RSS_FEEDS_JSON) ‚Üí –¥–µ—Ñ–æ–ª—Ç."""
    # 1) Per-user DB override
    if user_id:
        try:
            from models import RssFeedSettings
            row = RssFeedSettings.query.filter_by(user_id=user_id).first()
            if row and row.feeds:
                feeds = _normalize_feeds(row.feeds)
                if feeds:
                    return feeds
        except Exception:
            # If DB/app context isn't available, fall back to env/default.
            pass

    # 2) Global env override
    raw = os.environ.get('RSS_FEEDS_JSON', '').strip()
    if raw:
        try:
            parsed = json.loads(raw)
            feeds = _normalize_feeds(parsed)
            if feeds:
                return feeds
        except Exception:
            pass

    # 3) Hardcoded default
    return RSS_FEEDS

# –ö–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞ –¥–ª—è —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—ó —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç—É
RELEVANT_KEYWORDS = [
    # –ê–Ω–≥–ª—ñ–π—Å—å–∫–∞
    'tile', 'tiles', 'bathroom', 'bath', 'shower', 'renovation', 
    'interior', 'design', 'ceramic', 'porcelain', 'marble',
    'floor', 'flooring', 'wall', 'kitchen', 'sink', 'faucet',
    'modern', 'minimalist', 'luxury', 'trend', 'color',
    
    # –ù—ñ–º–µ—Ü—å–∫–∞
    'fliesen', 'bad', 'badezimmer', 'dusche', 'renovierung',
    'innendesign', 'keramik', 'marmor', 'boden', 'wand',
    'k√ºche', 'waschbecken', 'modern', 'luxus', 'trend', 'farbe',
    
    # –ó–∞–≥–∞–ª—å–Ω—ñ
    '2024', '2025', 'trend', 'new', 'neu', 'design'
]


def fetch_rss_feed(feed_key: str, feeds: Optional[Dict[str, Dict]] = None) -> List[Dict]:
    """
    –û—Ç—Ä–∏–º–∞—Ç–∏ —Å—Ç–∞—Ç—Ç—ñ –∑ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ—ó RSS-–ª–µ–Ω—Ç–∏
    
    Args:
        feed_key: –ö–ª—é—á –ª–µ–Ω—Ç–∏ –∑ RSS_FEEDS
        
    Returns:
        List[Dict] –∑—ñ —Å—Ç–∞—Ç—Ç—è–º–∏
    """
    feeds = feeds or get_rss_feeds_config()
    if feed_key not in feeds:
        return []
    
    feed_config = feeds[feed_key]
    
    try:
        feed = feedparser.parse(feed_config['url'])
        
        articles = []
        for entry in feed.entries[:20]:  # –û—Å—Ç–∞–Ω–Ω—ñ 20 —Å—Ç–∞—Ç–µ–π
            # –°–ø—Ä–æ–±–∞ –≤–∏—Ç—è–≥–Ω—É—Ç–∏ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è (—è–∫—â–æ RSS –π–æ–≥–æ –Ω–∞–¥–∞—î)
            image_url = None
            try:
                if hasattr(entry, 'media_content') and entry.media_content:
                    mc = entry.media_content[0]
                    if isinstance(mc, dict):
                        image_url = mc.get('url')
                if not image_url and hasattr(entry, 'media_thumbnail') and entry.media_thumbnail:
                    mt = entry.media_thumbnail[0]
                    if isinstance(mt, dict):
                        image_url = mt.get('url')
            except Exception:
                image_url = None

            # –î–∞—Ç–∞ –ø—É–±–ª—ñ–∫–∞—Ü—ñ—ó
            published = None
            if hasattr(entry, 'published_parsed') and entry.published_parsed:
                published = datetime(*entry.published_parsed[:6])
            
            # –ó–º—ñ—Å—Ç
            content_html = ''
            if hasattr(entry, 'summary') and entry.summary:
                content_html = entry.summary
            elif hasattr(entry, 'content') and entry.content:
                content_html = entry.content[0].value if entry.content else ''

            # –í–∏—Ç—è–≥—É—î–º–æ –ø–µ—Ä—à–µ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –∑ HTML, —è–∫—â–æ media_* –Ω–µ –±—É–ª–æ
            if not image_url and content_html:
                m = re.search(r'<img[^>]+src=[\"\']([^\"\']+)[\"\']', content_html, flags=re.IGNORECASE)
                if m:
                    image_url = m.group(1)
            
            # –û—á–∏—â–∞—î–º–æ HTML
            content = re.sub(r'<[^>]+>', '', content_html or '')
            
            articles.append({
                'title': entry.get('title', 'No title'),
                'link': entry.get('link', ''),
                'content': content[:500],  # –û–±–º–µ–∂—É—î–º–æ
                'published': published,
                'source': feed_config['name'],
                'category': feed_config['category'],
                'language': feed_config['language'],
                'image_url': image_url
            })
        
        return articles
        
    except Exception as e:
        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥—É {feed_key}: {e}")
        return []


def fetch_all_feeds(feeds: Optional[Dict[str, Dict]] = None) -> List[Dict]:
    """
    –û—Ç—Ä–∏–º–∞—Ç–∏ —Å—Ç–∞—Ç—Ç—ñ –∑ —É—Å—ñ—Ö RSS-–ª–µ–Ω—Ç
    
    Returns:
        List[Dict] –≤—Å—ñ—Ö —Å—Ç–∞—Ç–µ–π, –≤—ñ–¥—Å–æ—Ä—Ç–æ–≤–∞–Ω–∏—Ö –∑–∞ –¥–∞—Ç–æ—é
    """
    all_articles = []

    feeds = feeds or get_rss_feeds_config()
    
    for feed_key in feeds:
        print(f"üì° –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è {feeds[feed_key]['name']}...")
        articles = fetch_rss_feed(feed_key, feeds=feeds)
        all_articles.extend(articles)
        print(f"   ‚úÖ {len(articles)} —Å—Ç–∞—Ç–µ–π")
    
    # –°–æ—Ä—Ç—É—î–º–æ –∑–∞ –¥–∞—Ç–æ—é
    all_articles.sort(key=lambda x: x.get('published') or datetime.min, reverse=True)
    
    return all_articles


def filter_relevant_articles(articles: List[Dict], 
                             keywords: List[str] = None) -> List[Dict]:
    """
    –§—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∏—Ö —Å—Ç–∞—Ç–µ–π –∑–∞ –∫–ª—é—á–æ–≤–∏–º–∏ —Å–ª–æ–≤–∞–º–∏
    
    Args:
        articles: –°–ø–∏—Å–æ–∫ —Å—Ç–∞—Ç–µ–π
        keywords: –ö–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞ –¥–ª—è —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—ó (–∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º RELEVANT_KEYWORDS)
        
    Returns:
        List[Dict] –≤—ñ–¥—Ñ—ñ–ª—å—Ç—Ä–æ–≤–∞–Ω–∏—Ö —Å—Ç–∞—Ç–µ–π
    """
    if keywords is None:
        keywords = RELEVANT_KEYWORDS
    
    keywords_lower = [k.lower() for k in keywords]
    
    relevant = []
    for article in articles:
        text = f"{article['title']} {article['content']}".lower()
        
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ –º—ñ—Å—Ç–∏—Ç—å –±—É–¥—å-—è–∫–µ –∫–ª—é—á–æ–≤–µ —Å–ª–æ–≤–æ
        matches = [kw for kw in keywords_lower if kw in text]
        
        if matches:
            article['matched_keywords'] = matches
            article['relevance_score'] = len(matches)
            relevant.append(article)
    
    # –°–æ—Ä—Ç—É—î–º–æ –∑–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ñ—Å—Ç—é
    relevant.sort(key=lambda x: x['relevance_score'], reverse=True)
    
    return relevant


def get_trending_topics(days: int = 7, max_topics: int = 10, user_id: Optional[str] = None) -> List[Dict]:
    """
    –û—Ç—Ä–∏–º–∞—Ç–∏ –∞–∫—Ç—É–∞–ª—å–Ω—ñ —Ç—Ä–µ–Ω–¥–∏ –∑–∞ –æ—Å—Ç–∞–Ω–Ω—ñ N –¥–Ω—ñ–≤
    
    Args:
        days: –ö—ñ–ª—å–∫—ñ—Å—Ç—å –¥–Ω—ñ–≤ –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É
        max_topics: –ú–∞–∫—Å–∏–º—É–º —Ç–æ–ø—ñ–∫—ñ–≤
        
    Returns:
        List[Dict] —Ç—Ä–µ–Ω–¥–æ–≤–∏—Ö —Ç–µ–º
    """
    feeds = get_rss_feeds_config(user_id=user_id)
    all_articles = fetch_all_feeds(feeds=feeds)
    
    # –§—ñ–ª—å—Ç—Ä—É—î–º–æ –∑–∞ –¥–∞—Ç–æ—é
    cutoff = datetime.now() - timedelta(days=days)
    recent = [a for a in all_articles if a.get('published') and a['published'] > cutoff]
    
    # –§—ñ–ª—å—Ç—Ä—É—î–º–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ñ
    relevant = filter_relevant_articles(recent)
    
    return relevant[:max_topics]


def generate_content_ideas_from_trends(trends: List[Dict]) -> List[Dict]:
    """
    –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è —ñ–¥–µ–π –¥–ª—è –∫–æ–Ω—Ç–µ–Ω—Ç—É –Ω–∞ –æ—Å–Ω–æ–≤—ñ —Ç—Ä–µ–Ω–¥—ñ–≤
    (–ë–∞–∑–æ–≤–∞ –≤–µ—Ä—Å—ñ—è –±–µ–∑ AI)
    
    Args:
        trends: –°–ø–∏—Å–æ–∫ —Ç—Ä–µ–Ω–¥—ñ–≤
        
    Returns:
        List[Dict] —ñ–¥–µ–π –¥–ª—è –∫–æ–Ω—Ç–µ–Ω—Ç—É
    """
    ideas = []
    
    post_templates = [
        "üî• –¢—Ä–µ–Ω–¥: {title}\n\n–Ø–∫ –º–∏ –∑–∞—Å—Ç–æ—Å–æ–≤—É—î–º–æ —Ü–µ —É –Ω–∞—à–∏—Ö –ø—Ä–æ–µ–∫—Ç–∞—Ö –≤–∞–Ω–Ω–∏—Ö –∫—ñ–º–Ω–∞—Ç —É –§—Ä–∞–Ω–∫—Ñ—É—Ä—Ç—ñ!",
        "üí° –ù–∞—Ç—Ö–Ω–µ–Ω–Ω—è: {title}\n\n–ó–≤–µ—Ä–Ω—ñ—Ç—å—Å—è –¥–æ –Ω–∞—Å –¥–ª—è –≤—Ç—ñ–ª–µ–Ω–Ω—è —Å—É—á–∞—Å–Ω–∏—Ö —ñ–¥–µ–π —É –≤–∞—à—ñ–π –≤–∞–Ω–Ω—ñ–π!",
        "üìä –ù–æ–≤–∏–Ω–∫–∞ –≤ –¥–∏–∑–∞–π–Ω—ñ: {title}\n\n–ú–∏ —Å–ª—ñ–¥–∫—É—î–º–æ –∑–∞ —Ç—Ä–µ–Ω–¥–∞–º–∏ —â–æ–± –≤–∞—à–∞ –≤–∞–Ω–Ω–∞ –±—É–ª–∞ —Å—Ç–∏–ª—å–Ω–æ—é!",
        "üè† –Ü–¥–µ—è –¥–ª—è –≤–∞—à–æ–≥–æ –¥–æ–º—É: {title}\n\n–ë–µ–∑–∫–æ—à—Ç–æ–≤–Ω–∞ –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü—ñ—è –ø–æ –¥–∏–∑–∞–π–Ω—É –≤–∞–Ω–Ω–æ—ó!"
    ]
    
    for i, trend in enumerate(trends[:5]):
        template = post_templates[i % len(post_templates)]
        
        ideas.append({
            'source_trend': trend['title'],
            'source_link': trend['link'],
            'post_idea': template.format(title=trend['title'][:100]),
            'hashtags': [
                '#fliesen', '#badsanierung', '#frankfurt',
                '#bathroom', '#design', '#trend', '#renovierung',
                '#interiordesign', '#home', '#inspiration'
            ],
            'content_type': 'trend_based',
            'keywords': trend.get('matched_keywords', [])
        })
    
    return ideas


# –¢–µ—Å—Ç
if __name__ == '__main__':
    print("üß™ –¢–µ—Å—Ç RSS Service...")
    
    print("\nüì° –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ç—Ä–µ–Ω–¥—ñ–≤...")
    trends = get_trending_topics(days=14, max_topics=5)
    
    print(f"\nüìä –ó–Ω–∞–π–¥–µ–Ω–æ {len(trends)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∏—Ö —Å—Ç–∞—Ç–µ–π:")
    for i, trend in enumerate(trends[:5], 1):
        print(f"\n{i}. {trend['title'][:80]}...")
        print(f"   üìç –î–∂–µ—Ä–µ–ª–æ: {trend['source']}")
        print(f"   üè∑Ô∏è –ö–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞: {', '.join(trend.get('matched_keywords', [])[:5])}")
    
    print("\nüí° –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è —ñ–¥–µ–π –¥–ª—è –∫–æ–Ω—Ç–µ–Ω—Ç—É...")
    ideas = generate_content_ideas_from_trends(trends)
    
    for idea in ideas[:3]:
        print(f"\nüìù {idea['post_idea'][:150]}...")
